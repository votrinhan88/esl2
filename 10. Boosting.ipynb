{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Literal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Literal, Sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, task:Literal['r', 'c'] = 'c',\n",
    "                       depth:int = None,\n",
    "                       path:str = '',\n",
    "                       parent = None):\n",
    "        # Arguments\n",
    "        self.task        = task\n",
    "        self.depth       = depth\n",
    "        self.path        = path\n",
    "        self.parent:Node = parent\n",
    "        # Pre-allocate\n",
    "        self.is_leaf = False\n",
    "        self.info:Tensor = None\n",
    "        self.feature:Tensor = None\n",
    "        self.threshold:Tensor = None\n",
    "        self.children:Sequence[Node] = []\n",
    "        # Based on task (Classification/Regression)\n",
    "        self.value:Tensor = None\n",
    "        if self.task == 'c':\n",
    "            self.distr:Tensor = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Magic attribute to get class name\n",
    "        if self.depth == 0:\n",
    "            return f\"{self.__class__.__name__}-{self.task} Root, I = {self.info:.3f}, value = {round(self.value, 2)}\"\n",
    "        elif self.depth > 0:\n",
    "            return f\"{self.__class__.__name__}-{self.task} {self.path}, I = {self.info:.3f}, value = {round(self.value, 2)}\"\n",
    "\n",
    "    def branch(self):\n",
    "        left_node  = Node(task = self.task, depth = self.depth + 1, path = self.path + 'L', parent = self)\n",
    "        right_node = Node(task = self.task, depth = self.depth + 1, path = self.path + 'R', parent = self)\n",
    "        return left_node, right_node\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self,\n",
    "        task:Literal['c', 'r']='c',\n",
    "        method_info:Literal['gini', 'entropy', 'var', 'std']='gini',\n",
    "        max_depth:int=4,\n",
    "        drop_features:bool=True,\n",
    "    ):\n",
    "        assert ((task == 'c') & (method_info in ['gini', 'entropy']) |\n",
    "                (task == 'r') & (method_info in ['var', 'std'])), \\\n",
    "               \"Use task 'c' with method_info 'gini' or 'entropy', or task 'r' with method_info 'var' or 'std'\"\n",
    "        self.task          = task\n",
    "        self.method_info   = method_info\n",
    "        self.max_depth     = max_depth\n",
    "        self.drop_features = drop_features\n",
    "\n",
    "        self.depth:int = 0\n",
    "        self.feature_picks:Tensor = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"DT-{self.task} with depth {self.depth} (max {self.max_depth}), drop features {'ON' if self.drop_features else 'OFF'}\"\n",
    "\n",
    "    def fit(self, X_train:Tensor, y_train:Tensor):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.num_features = X_train.shape[1]\n",
    "        self.feature_picks = torch.arange(self.num_features)\n",
    "\n",
    "        self.root = Node(depth = 0, task = self.task)\n",
    "\n",
    "        if self.task == 'c':\n",
    "            self.num_classes = len(y_train.unique())\n",
    "            self.feed = self.feed_c\n",
    "            self.compute_info = self.compute_info_c\n",
    "        elif self.task == 'r':\n",
    "            self.feed = self.feed_r\n",
    "            self.compute_info = self.compute_info_r\n",
    "\n",
    "        idx_root = torch.arange(self.X_train.size()[0])\n",
    "        self.feed(node=self.root, idx_label=idx_root)\n",
    "        self.grow(node=self.root, idx_node=idx_root)\n",
    "\n",
    "    def grow(self, node:Node, idx_node:Tensor):\n",
    "        max_gain = self.find_best_split(node, idx_node)\n",
    "        if max_gain > 0:\n",
    "            # Continue growing\n",
    "            self.depth = max(self.depth, node.depth + 1)\n",
    "            left_node, right_node = node.branch()\n",
    "            node.children = [left_node, right_node]\n",
    "            # Re-split data by optimal feature and threshold for children nodes\n",
    "            idx_left, idx_right = self.split(idx_node, feature = node.feature, threshold = node.threshold)\n",
    "            # Leaf node:\n",
    "            #  - Has a unique class distribution (contains only one class)\n",
    "            #  - OR has reached max depth\n",
    "            for (branch, idx_branch) in zip([left_node, right_node], [idx_left, idx_right]):\n",
    "                self.feed(branch, idx_branch)\n",
    "                y_branch = self.y_train[idx_branch]\n",
    "                if (len(y_branch.unique()) == 1) | (branch.depth == self.max_depth):\n",
    "                    branch.is_leaf = True\n",
    "                else:\n",
    "                    self.grow(branch, idx_branch)\n",
    "    \n",
    "    def find_best_split(self, node:Node, idx_node:Tensor) -> Tensor:\n",
    "        X_node = self.X_train[idx_node]\n",
    "        # Select random features (sqrt() of previous node's num_features)\n",
    "        if self.drop_features == True:\n",
    "            shuffle_idx = torch.randperm(self.feature_picks.numel())\n",
    "            self.feature_picks = self.feature_picks.view(-1)[shuffle_idx].view(self.feature_picks.size())\n",
    "            self.feature_picks = self.feature_picks[0:int(self.feature_picks.numel()**0.5)]\n",
    "        # Split based on the reduced (or not) set of features \n",
    "        max_gain = -torch.tensor(float('inf'))\n",
    "        for feature in torch.arange(self.num_features):\n",
    "            # thresholds = torch.linspace(start = X_node[:, feature].min(),\n",
    "            #                             end = X_node[:, feature].max(),\n",
    "            #                             steps = self.num_splits + 2)[1:self.num_splits+1]\n",
    "            uniques = X_node[:, feature].sort()[0].unique()\n",
    "            thresholds = (uniques[1:] + uniques[:-1])/2\n",
    "            for threshold in thresholds:\n",
    "                idx_left, idx_right = self.split(\n",
    "                    idx_node=idx_node,\n",
    "                    feature=feature,\n",
    "                    threshold=threshold\n",
    "                )\n",
    "                gain = self.compute_gain(\n",
    "                    node=node,\n",
    "                    idx_node=idx_node,\n",
    "                    idx_left=idx_left,\n",
    "                    idx_right=idx_right\n",
    "                )\n",
    "                if gain > max_gain:\n",
    "                    max_gain = gain\n",
    "                    opt_feature = feature\n",
    "                    opt_threshold = threshold        \n",
    "        # Update node with optimal split\n",
    "        node.feature = opt_feature\n",
    "        node.threshold = opt_threshold\n",
    "        return max_gain\n",
    "\n",
    "    def split(self, idx_node:Tensor, feature:Tensor, threshold:Tensor) -> Sequence[Tensor]:\n",
    "        X_node = self.X_train[idx_node]\n",
    "\n",
    "        left_ind = X_node[:, feature] <= threshold\n",
    "        idx_left = idx_node[left_ind]\n",
    "        idx_right = idx_node[~left_ind]\n",
    "        return idx_left, idx_right\n",
    "\n",
    "    def compute_gain(self, node:Node, idx_node:Tensor, idx_left:Tensor, idx_right:Tensor) -> Tensor:\n",
    "        left_info = self.compute_info(idx_left)\n",
    "        right_info = self.compute_info(idx_right)\n",
    "        \n",
    "        gain = node.info - (idx_left.numel()*left_info + idx_right.numel()*right_info)/idx_node.numel()\n",
    "        \n",
    "        # w_node = self.weights[idx_node].sum()\n",
    "        # w_left = self.weights[idx_left].sum()\n",
    "        # w_right = self.weights[idx_right].sum()\n",
    "        # gain = w_node*node.info - (w_left/w_node*left_info + w_right/w_node*right_info)\n",
    "        return gain\n",
    "\n",
    "    def feed_c(self, node:Node, idx_label:Tensor):\n",
    "        label = self.y_train[idx_label]\n",
    "        node.info = self.compute_info_c(idx_label)\n",
    "        onehot:Tensor = F.one_hot(label.squeeze(dim = 1), num_classes = self.num_classes)\n",
    "        \n",
    "        node.distr = onehot.sum(dim = 0)\n",
    "\n",
    "        # node.distr = (self.weights[idx_label]*onehot).sum(dim = 0)\n",
    "        # node.distr = node.distr/node.distr.sum()\n",
    "\n",
    "        node.value = node.distr.max(dim = 0)[1]\n",
    "\n",
    "    def feed_r(self, node:Node, idx_label:Tensor):\n",
    "        label = self.y_train[idx_label]\n",
    "        node.info = self.compute_info_r(idx_label)\n",
    "        node.value = label.mean()\n",
    "    \n",
    "    def compute_info_c(self, idx_label:Tensor) -> Tensor:\n",
    "        label = self.y_train[idx_label]\n",
    "        \n",
    "        # Classification: reducing Gini impurity or entropy\n",
    "        onehot:Tensor = F.one_hot(label.squeeze(dim = 1), num_classes = self.num_classes)\n",
    "        distr = onehot.sum(dim = 0)\n",
    "        distr = distr/distr.sum()\n",
    "        if self.method_info == 'gini':\n",
    "            info = 1 - (distr**2).sum()\n",
    "        elif self.method_info == 'entropy':\n",
    "            # Side note:\n",
    "            # 1. Entropy of a system (all classes) = sum of entropy of its parts (each class)\n",
    "            # 2. Moreover, if a class has no examples, it is absolutely predictable absent, hence its\n",
    "            #   entropy is 0.\n",
    "            # 3. To ease dealing with absent classes (which yields log(0) = -inf), and (2.), the \n",
    "            #   computation only considers existing classes.\n",
    "            info = -(distr[distr != 0]*distr[distr != 0].log()).sum()\n",
    "        return info\n",
    "    \n",
    "    def compute_info_r(self, idx_label:Tensor) -> Tensor:\n",
    "        label = self.y_train[idx_label]\n",
    "\n",
    "        # Regression: reducing Variance or Standard deviation\n",
    "        if self.method_info == 'var':\n",
    "            info = ((label - label.mean())**2).sum()/label.size()[0]\n",
    "        elif self.method_info == 'std':\n",
    "            info = ((label - label.mean())**2).sum().sqrt()/label.size()[0]\n",
    "        return info\n",
    "\n",
    "    def print_tree(self):\n",
    "        def traverse_print(node: Node):\n",
    "            # Print node\n",
    "            if node.depth == 0:\n",
    "                print(f\"Root:\")\n",
    "            elif node.depth > 0:\n",
    "                print(f\"{'    '*node.depth}Branch {node.path} \" +\n",
    "                      f\"(x{node.parent.feature.item()} {'≤' if node.path[-1] == 'L' else '>'} {node.parent.threshold.item():.2f}):\" +\n",
    "                      f\"{f' {node.distr.numpy()}' if self.task == 'c' else ''}\" +\n",
    "                      f\"{f' = {round(node.value.item(), 2)}' if node.is_leaf else ''}\")\n",
    "            # Go to children branches\n",
    "            if node.is_leaf == False:\n",
    "                for branch in node.children:\n",
    "                    traverse_print(branch)\n",
    "            \n",
    "        traverse_print(self.root)\n",
    "\n",
    "    def forward(self, input:Tensor, method = 'all') -> Tensor:\n",
    "        if method == 'each':\n",
    "            # Method 1: Loop and traverse each example through the tree\n",
    "            def traverse_forward(node:Node, input:Tensor, yhat) -> Tensor:\n",
    "                if yhat is not None:\n",
    "                    return yhat\n",
    "                elif yhat is None:\n",
    "                    if node.is_leaf == True:\n",
    "                        return node.value\n",
    "                    elif node.is_leaf == False:\n",
    "                        if input[node.feature] < node.threshold:\n",
    "                            return traverse_forward(node.children[0], input, yhat)\n",
    "                        elif input[node.feature] >= node.threshold:\n",
    "                            return traverse_forward(node.children[1], input, yhat)\n",
    "            \n",
    "            yhat = {\n",
    "                'c':-torch.ones(size=[input.shape[0], 1], dtype=torch.long),\n",
    "                'r':torch.zeros(size=[input.shape[0], 1], dtype=torch.float),\n",
    "            }.get(self.task)\n",
    "            for example in torch.arange(input.size()[0]):\n",
    "                yhat[example] = traverse_forward(self.root, input[example, :], yhat = None)\n",
    "            return yhat\n",
    "\n",
    "        elif method == 'all':\n",
    "            # Method 2: Traverse all examples through the tree at once\n",
    "            def traverse_forward(node:Node, input:Tensor, yhat:Tensor, yhat_id:Tensor) -> Tensor:\n",
    "                if node.is_leaf == True:\n",
    "                    yhat[yhat_id.squeeze()] = node.value\n",
    "                    return yhat\n",
    "                elif node.is_leaf == False:\n",
    "                    left_ind = input[:, node.feature] < node.threshold\n",
    "                    left_input, right_input = input[left_ind, :], input[~left_ind, :]\n",
    "                    idx_left, idx_right = yhat_id[left_ind, :], yhat_id[~left_ind, :]\n",
    "                    for branch, branch_input, idx_branch in zip(node.children, [left_input, right_input], [idx_left, idx_right]):\n",
    "                        if len(idx_branch) > 0:\n",
    "                            yhat = traverse_forward(branch, branch_input, yhat, idx_branch)\n",
    "                    return yhat\n",
    "            \n",
    "            yhat = {\n",
    "                'c':-torch.ones(size=[input.shape[0], 1], dtype=torch.long),\n",
    "                'r':torch.zeros(size=[input.shape[0], 1], dtype=torch.float),\n",
    "            }.get(self.task)\n",
    "            yhat_id = torch.arange(input.size()[0]).unsqueeze(dim = 1)\n",
    "            yhat = traverse_forward(self.root, input, yhat, yhat_id)\n",
    "            return yhat\n",
    "\n",
    "\n",
    "class Boost():\n",
    "    def __init__(self,\n",
    "        n_trees:int=10,\n",
    "        task:Literal['c', 'r'] = 'c',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"_summary_.\n",
    "\n",
    "        Args:\n",
    "        + `n_trees`: Number of trees. Defaults to `10`.\n",
    "\n",
    "        Kwargs: Arguments for `DecisionTree`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_trees = n_trees\n",
    "        self.task = task\n",
    "        self.kwargs = kwargs\n",
    "        self.trees = [DecisionTree(task=self.task, **kwargs) for i in range(self.n_trees)]\n",
    "\n",
    "    def forward(self, input:Tensor) -> Tensor:\n",
    "        pred = torch.zeros(size=[input.shape[0], 1])\n",
    "        for tree in self.trees:\n",
    "            pred_i = tree.forward(input)\n",
    "            pred += pred_i\n",
    "        return pred\n",
    "\n",
    "    def fit(self, X_train:Tensor, y_train:Tensor):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.num_features = X_train.shape[1]\n",
    "\n",
    "        pred = torch.zeros_like(self.y_train)\n",
    "        residual = self.y_train.clone().detach()\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            tree.fit(X_train=X_train, y_train=residual)\n",
    "            pred_i = tree.forward(X_train)\n",
    "            pred += pred_i\n",
    "            residual = y_train - pred\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        args = {\n",
    "            'task':self.task,\n",
    "            'n_trees':self.n_trees,\n",
    "            **self.kwargs,\n",
    "        }\n",
    "        args = ', '.join([f'{k}={v}' for k, v in args.items() if v is not None])\n",
    "        return f'{self.__class__.__name__}({args})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Regression on 1D Signal\n",
    "num_observed = 50\n",
    "print('Regression Tree on 1D signal.')\n",
    "print(f'Number of observed values: {num_observed}')\n",
    "# Generate dummy data\n",
    "def signal(input):\n",
    "    return 0.6*input + 1.5*torch.sin(input)\n",
    "X_train = 10*torch.rand([num_observed, 1])\n",
    "y_train = signal(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12, 6), constrained_layout=True, squeeze=False)\n",
    "for i, n_trees in enumerate([1, 3, 5, 10, 15, 20]):\n",
    "    row, col = i // 3, i % 3\n",
    "    h = Boost(\n",
    "        n_trees=n_trees,\n",
    "        task='r',\n",
    "        method_info='var',\n",
    "        max_depth=2,\n",
    "        drop_features=False,\n",
    "    )\n",
    "    h.fit(X_train=X_train, y_train=y_train)\n",
    "\n",
    "    X_test = torch.arange(start=-2, end=12, step=0.01).unsqueeze(dim = 1)\n",
    "    y_hat = h.forward(X_test)\n",
    "    residual = signal(X_test) - y_hat\n",
    "\n",
    "    ax[row, col].step(X_test, y_hat,\n",
    "            linewidth=2, color='blue',\n",
    "            label=f'n_trees={n_trees}')\n",
    "    \n",
    "    ax[row, col].scatter(X_train, y_train,\n",
    "            color='black', s=40, label='Observed')\n",
    "    ax[row, col].plot(X_test, signal(X_test),\n",
    "            linewidth=1, linestyle='dashed', color='red', alpha=0.5, label='Ground truth')\n",
    "    ax[row, col].set(title = f'n_trees={n_trees}, residual={residual.mean(dim=0).item():.4f}')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6), constrained_layout=True, squeeze=False)\n",
    "\n",
    "n_trees = [1, 3, 5, 7, 10, 15, 20, 30, 50]\n",
    "residuals_train = []\n",
    "residuals_test = []\n",
    "\n",
    "for i, n in enumerate(n_trees):\n",
    "    h = Boost(\n",
    "        n_trees=n,\n",
    "        task='r',\n",
    "        method_info='var',\n",
    "        max_depth=1,\n",
    "        drop_features=False,\n",
    "    )\n",
    "    h.fit(X_train=X_train, y_train=y_train)\n",
    "\n",
    "    X_test = torch.arange(start=-2, end=12, step=0.01).unsqueeze(dim = 1)\n",
    "    y_hat = h.forward(X_test)\n",
    "    residual = signal(X_test) - y_hat\n",
    "\n",
    "    residuals_train.append((h.forward(X_train) - y_train).mean(dim=0))\n",
    "    residuals_test.append((h.forward(X_test) - signal(X_test)).mean(dim=0))\n",
    "\n",
    "ax[0, 0].plot(n_trees, residuals_train, label='train')\n",
    "ax[0, 0].plot(n_trees, residuals_test, label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
