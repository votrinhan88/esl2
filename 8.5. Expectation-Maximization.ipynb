{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'figure.titlesize': 12,\n",
    "    'axes.titlesize':   10,\n",
    "    'axes.labelsize':   10,\n",
    "    'font.size':        8,\n",
    "    'xtick.labelsize':  8,\n",
    "    'ytick.labelsize':  8,\n",
    "    'legend.fontsize':  8,\n",
    "    'lines.linewidth':  1,\n",
    "})\n",
    "\n",
    "COLORS = ['red', 'blue', 'green', 'orange', 'purple',\n",
    "          'brown', 'pink', 'gray', 'olive', 'cyan',\n",
    "          'tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:purple',\n",
    "          'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">1-D data (Salary)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 500\n",
    "NUM_CLASSES = 2\n",
    "TIER_THRESHOLDS = [-0.5, 0.5]\n",
    "NUM_EPOCHS = 500\n",
    "BOUNDARIES = [[-3, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/Salary_Data.csv')\n",
    "X_train = torch.tensor(dataset.iloc[:, 1].values, dtype=torch.float).unsqueeze(dim=1)\n",
    "# Normalize data\n",
    "X_mean = X_train.mean(dim=0)\n",
    "X_std = X_train.std(dim=0)\n",
    "X_train = (X_train - X_mean)/X_std\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3), constrained_layout=True)\n",
    "ax.set(\n",
    "    xlim=BOUNDARIES[0],\n",
    "    xlabel='Salary',\n",
    "    title='Salary Distribution (Normalized)'\n",
    ")\n",
    "ax.hist(X_train.squeeze(dim=1), bins=50, range=(-3, 3))\n",
    "ax.scatter(\n",
    "    x=X_train.squeeze(dim=1),\n",
    "    y=torch.zeros_like(X_train.squeeze(dim=1)),\n",
    "    color='black'\n",
    ")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have labels?  \n",
    "\n",
    "Assuming:\n",
    "+ Salary < -0.5 standard deviation are the poor\n",
    "+ Salary > 0.5 standard deviation are the rich\n",
    "+ Others: mixed\n",
    "\n",
    "With labels, we can easily construct the distribution by calculating the mean and standard deviation for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier = -torch.ones_like(X_train, dtype=torch.long)\n",
    "tier[X_train<TIER_THRESHOLDS[0]] = 0\n",
    "tier[X_train>=TIER_THRESHOLDS[1]] = 1\n",
    "mid_tier = ((X_train>=TIER_THRESHOLDS[0]) & (X_train<TIER_THRESHOLDS[1])).nonzero().squeeze(dim=1)\n",
    "tier[mid_tier] = torch.randint(low=0, high=NUM_CLASSES, size=tier[mid_tier].shape)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3), constrained_layout=True)\n",
    "ax.set(\n",
    "    xlim=BOUNDARIES[0],\n",
    "    xlabel='Salary',\n",
    "    title='Salary Distribution (labeled data)'\n",
    ")\n",
    "x_plot = torch.linspace(*BOUNDARIES[0], steps=RESOLUTION).unsqueeze(dim=1)\n",
    "for k in range(NUM_CLASSES):\n",
    "    ax.scatter(\n",
    "        x=X_train[tier==k],\n",
    "        y=torch.zeros_like(X_train[tier==k]), \n",
    "        color=COLORS[k],\n",
    "    )\n",
    "\n",
    "    # continue # Comment out to see Gaussians\n",
    "    gaussian = Normal(\n",
    "        loc=X_train[tier==k].mean(dim=0),\n",
    "        scale=X_train[tier==k].std(dim=0),\n",
    "    )\n",
    "    y_plot = gaussian.log_prob(value=x_plot).exp()\n",
    "    ax.plot(x_plot, y_plot, color=COLORS[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our Unsupervised problem  \n",
    "(Gaussian Mixture sometimes cannot find solutions, if so rerun the block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3), constrained_layout=True)\n",
    "ax.set(\n",
    "    xlim=BOUNDARIES[0],\n",
    "    xlabel='Salary',\n",
    "    title='Salary Distribution (Normalized) - Gaussian Mixture from sklearn'\n",
    ")\n",
    "ax.hist(X_train.squeeze(dim=1), bins=50, range=BOUNDARIES[0])\n",
    "\n",
    "gauss_mix = GaussianMixture(n_components=2, init_params='random')\n",
    "gauss_mix.fit(X_train)\n",
    "y_gm = torch.tensor(gauss_mix.predict(X_train)).unsqueeze(dim=1)\n",
    "ax.plot(x_plot, torch.zeros_like(x_plot), color='black')\n",
    "for k in range(NUM_CLASSES):\n",
    "    ax.scatter(x=X_train[y_gm==k], y=torch.zeros_like(X_train[y_gm==k]), color=COLORS[k])\n",
    "\n",
    "    # continue # Comment out to see Gaussians\n",
    "    gaussian = Normal(\n",
    "        loc=torch.tensor(gauss_mix.means_[k]).squeeze(),\n",
    "        scale=torch.tensor(gauss_mix.covariances_[k]).squeeze().sqrt(),\n",
    "    )\n",
    "    y_plot = gaussian.log_prob(value=x_plot).exp()\n",
    "    ax.plot(x_plot, y_plot, color=COLORS[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering\n",
    "\n",
    "**Inputs:** Data $X$, number of clusters $K$  \n",
    "**Outputs:** Centroids $\\{m^{(k)}\\}_{k=1}^K$  \n",
    "**Algorithm**:\n",
    "1. Init centroids $\\{m^{(k)}\\}_{k=1}^K$ to random points in $X$\n",
    "2. WHILE (not converge):\n",
    "    1. Assignment step: Assign each observation to the nearest cluster (centroid)\n",
    "        + Points belong to the $k$-th cluster: $S^{(k)} = \\{x_p:\\|x_p-m^{(k)}\\|^2 \\le \\|x_p-m^{(j)}\\|^2 \\forall j, 1\\le j\\le K\\}$\n",
    "    2. Update step: Recalculate centroids with current points in the cluster\n",
    "        + $m^{(k)} = \\frac{1}{\\|S^{(k)}\\|} \\sum_{x_j \\in S^{(k)}}{x_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClassifier:\n",
    "    def __init__(self, X_train:Tensor, K:int):\n",
    "        self.X_train = X_train\n",
    "        self.K = K\n",
    "\n",
    "        self.centroids:Tensor = self.X_train[torch.randperm(self.X_train.shape[0])][range(self.K)]\n",
    "    \n",
    "    # Fix centroids, update labels\n",
    "    def forward(self, X_train:Tensor) -> Tensor:\n",
    "        distance = torch.zeros([X_train.shape[0], self.K])\n",
    "        for k in torch.arange(self.K):\n",
    "            distance[:, k] = torch.sqrt(torch.sum((self.centroids[k, :] - X_train)**2, dim=1))\n",
    "        \n",
    "        (_, yhat) = torch.min(distance, dim=1, keepdim=True)        \n",
    "        return yhat\n",
    "\n",
    "    # Fix labels, update centroids\n",
    "    def backward(self, X_train:torch.Tensor, yhat:torch.Tensor):\n",
    "        for k in torch.arange(self.K):\n",
    "            self.centroids[k, :] = torch.mean(X_train[yhat.squeeze() == k, :], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 3), constrained_layout=True)\n",
    "fig.suptitle('Salary Distribution (Normalized) - K-means')\n",
    "ax.set(xlim=BOUNDARIES[0])\n",
    "\n",
    "ax.hist(X_train.squeeze(dim=1), bins=50, range=(-3, 3))\n",
    "km = KMeansClassifier(X_train=X_train, K=2)\n",
    "for j in range(NUM_EPOCHS):\n",
    "    y_km = km.forward(X_train=X_train)\n",
    "    km.backward(X_train=X_train, yhat=y_km)\n",
    "y_km = km.forward(X_train)\n",
    "\n",
    "for k in range(km.K):\n",
    "    ax.scatter(x=X_train[y_km==k], y=torch.zeros_like(X_train[y_km==k]), color=COLORS[k])\n",
    "\n",
    "    # gaussian = MultivariateNormal(loc=mus[k], covariance_matrix=sigmas[k])\n",
    "    # y_plot = gaussian.log_prob(value=x_plot.unsqueeze(dim=1)).exp()\n",
    "    # ax.plot(x_plot, pis[k]*y_plot*5, color=COLORS[k])\n",
    "    ax.axvline(x=km.centroids[k], color=COLORS[k], label=f'Centroid {k}')\n",
    "\n",
    "ax.step(x_plot.squeeze(dim=1), km.forward(x_plot).squeeze(dim=1), color='black', label='Class')\n",
    "ax.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation-Maximization for 1D Gaussian Mixture\n",
    "\n",
    "**Inputs:** Data $X_{N\\times d}$, number of Gaussians $K$, priors $\\{\\pi^{(k)}\\}_{k=1}^K$  \n",
    "**Outputs:** Gaussians $\\{\\theta^{(k)}\\}_{k=1}^K$, where $\\theta^{(k)} = \\{\\mu^{(k)}, \\sigma^{(k)}\\}$  \n",
    "**Algorithm**:\n",
    "1. Init Gaussians $\\{\\theta^{(k)}\\}_{k=1}^K$\n",
    "2. WHILE (not converge):\n",
    "    1. Expectation step\n",
    "    2. Maximization step\n",
    "\n",
    "**Expectation step:**  \n",
    "Construct Gaussian $\\{\\theta^{(k)}\\}_{k=1}^K$  \n",
    "Compute the responsibilities matrix $R_{N\\times K}$, i.e., **class-specific weights** for each sample\n",
    "$$R_{i, k} = \\frac{\\pi^{(k)} \\phi_{\\theta^{(k)}}(x_i)}\n",
    "                  {\\sum_{k'=1}^K {\\pi^{(k')} \\phi_{\\theta^{(k')}}(x_i)}}$$\n",
    "\n",
    "**Maximization step:**  \n",
    "Update the weighted mean, weighted variance, and (optionally) priors for the Gaussians:\n",
    "- Mean: $\\mu^{(k)} = \\frac{R_{:, k}\\times X}{\\sum{R_{:, k}}}$\n",
    "- Variance: ${\\sigma^{(k)}}^2 = \\frac{R_{:, k}\\times (X - \\mu^{(k)})^2}{\\sum{R_{:, k}}}$\n",
    "- Prior: $\\pi^{(k)} = \\frac{R_{:, k}}{N}$\n",
    "\n",
    "**Side note**:\n",
    "1. $\\mu^{(i)}$ are initialized to random points sampled from a standard Normal distribution\n",
    "2. $\\sigma^{(i)}$ are initialized to standard deviation of $X$\n",
    "3. $\\pi^{(i)}$ are initialized to $1/k$\n",
    "4. $\\pi^{(i)}$ may or may not be trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectationMaximization:\n",
    "    \"\"\"The Expectation-Maximization algorithm with Gaussian Mixture.\n",
    "\n",
    "    Args:\n",
    "    + `X_train`: Input data of shape [N * d].\n",
    "    + `K`: Number of Gaussians.\n",
    "    + `mus`: Mean of Gaussians, must be of shape [K * d]. Defaults to `None`,   \\\n",
    "        leave to initialize from a standard normal distribution.\n",
    "    + `Sigmas`: Covariance matrix of Gaussians, must be of shape [K * d * d].   \\\n",
    "        Defaults to `None`, leave to initialize as the covariance matrix of     \\\n",
    "        `X_train` for each Gaussian.\n",
    "    + `pis`: Prior of Gaussians, must be of shape [K]. Defaults to `None`, leave\\\n",
    "        to initialize as `1/K` for each Gaussian.\n",
    "    + `trainable_mus`: Flag to set `mus` trainable. Defaults to `True`.\n",
    "    + `trainable_Sigmas`: Flag to set `Sigmas` trainable. Defaults to `True`.\n",
    "    + `trainable_pis`: Flag to set `pis` trainable. Defaults to `True`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train:Tensor,\n",
    "        K:int,\n",
    "        mus:Optional[Sequence[float]]=None,\n",
    "        Sigmas:Optional[Sequence[float]]=None,\n",
    "        pis:Optional[Sequence[float]]=None,\n",
    "        trainable_mus:bool=True,\n",
    "        trainable_Sigmas:bool=True,\n",
    "        trainable_pis:bool=True,\n",
    "    ):\n",
    "        self.X_train = X_train\n",
    "        self.d = self.X_train.shape[1]\n",
    "        self.K = K\n",
    "        self.trainable_mus = trainable_mus\n",
    "        self.trainable_Sigmas = trainable_Sigmas\n",
    "        self.trainable_pis = trainable_pis\n",
    "\n",
    "        if mus is None:\n",
    "            self.mus = torch.normal(mean=0, std=1, size=[self.K, self.d])\n",
    "        else:\n",
    "            self.mus = torch.tensor(mus)\n",
    "\n",
    "        if Sigmas is None:\n",
    "            self.Sigmas = self.X_train.t().cov()\n",
    "            # Ensure Sigmas is of shape [k, d, d]\n",
    "            if self.d == 1:\n",
    "                self.Sigmas = self.Sigmas.unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "            self.Sigmas = self.Sigmas.tile(dims=[self.K, 1, 1])\n",
    "        else:\n",
    "            self.Sigmas = torch.tensor(Sigmas)\n",
    "\n",
    "        if pis is None:\n",
    "            self.pis = torch.ones(size=[self.K])/self.K\n",
    "        else:\n",
    "            self.pis = torch.tensor(pis)\n",
    "            # Normalize: sum of priors is 1\n",
    "            self.pis = self.pis/self.pis.sum(dim=0, keepdim=True)\n",
    "\n",
    "        self.responsibility = torch.zeros(size=[self.X_train.shape[0], self.K])\n",
    "        self.gaussians = [MultivariateNormal(loc=self.mus[k], covariance_matrix=self.Sigmas[k]) for k in range(self.K)]\n",
    "    \n",
    "    def expectation_step(self) -> Tensor:\n",
    "        for k in range(self.K):\n",
    "            self.responsibility[:, k] = self.pis[k]*(self.gaussians[k].log_prob(self.X_train).exp())\n",
    "        # Normalize: sum of prob. for each data point is 1\n",
    "        self.responsibility = self.responsibility/self.responsibility.sum(dim=1, keepdim=True)\n",
    "        return self.responsibility\n",
    "\n",
    "    def maximization_step(self) -> (Tensor, Tensor, Tensor):\n",
    "        for k in range(self.K):\n",
    "            mean = (self.responsibility[:, [k]]*self.X_train).sum(dim=0)/self.responsibility[:, [k]].sum(dim=0)\n",
    "            covariance = (self.responsibility[:, [k]]*(self.X_train - mean)).t()@(self.X_train - mean) \\\n",
    "                         /self.responsibility[:, [k]].sum(dim=0)\n",
    "\n",
    "            if self.trainable_mus == True:\n",
    "                self.mus[k] = mean\n",
    "            if self.trainable_Sigmas == True:\n",
    "                self.Sigmas[k] = covariance\n",
    "            if self.trainable_pis is True:\n",
    "                prior = self.responsibility[:, [k]].mean(dim=0)\n",
    "                self.pis[k] = prior\n",
    "        \n",
    "        # Update Gaussians\n",
    "        self.gaussians = [MultivariateNormal(loc=self.mus[k], covariance_matrix=self.Sigmas[k]) for k in range(self.K)]\n",
    "        return self.mus, self.Sigmas, self.pis\n",
    "    \n",
    "    def predict(self, input:Tensor) -> Tensor:\n",
    "        pred = torch.zeros(size=[input.shape[0], self.K])\n",
    "\n",
    "        for k in range(self.K):\n",
    "            pred[:, k] = self.pis[k]*(self.gaussians[k].log_prob(input).exp())\n",
    "        # Normalize: sum of prob. for each data point is 1\n",
    "        pred = pred/pred.sum(dim=1, keepdim=True)\n",
    "        return pred\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        params = {\n",
    "            'X_train':          self.X_train,\n",
    "            'K':                self.K,\n",
    "            'mus':              self.mus,\n",
    "            'Sigmas':           self.Sigmas,\n",
    "            'pis':              self.pis,\n",
    "            'trainable_mus':    self.trainable_mus,\n",
    "            'trainable_Sigmas': self.trainable_Sigmas,\n",
    "            'trainable_pis':    self.trainable_pis,\n",
    "        }\n",
    "        return f\"{self.__class__.__name__}({', '.join([f'{k}={v}' for k, v in params.items() if v is not None])})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), constrained_layout=True, squeeze=False, sharex='all', sharey='all')\n",
    "fig.suptitle('Salary Distribution (Normalized) - Expectation-Maximization algorithm')\n",
    "ax[0, 0].set(xlim=BOUNDARIES[0])\n",
    "\n",
    "for i, trainable_pis in enumerate([False, True]):\n",
    "    ax[i, 0].hist(X_train.squeeze(dim=1), bins=50, range=BOUNDARIES[0])\n",
    "    em = ExpectationMaximization(X_train=X_train, K=4, trainable_pis=trainable_pis, pis=[1, 1, 1, 1])\n",
    "    for j in range(500):\n",
    "        r = em.expectation_step()\n",
    "        mus, sigmas, pis = em.maximization_step()\n",
    "    pred = em.predict(X_train)\n",
    "\n",
    "    for k in range(em.K):\n",
    "        ax[i, 0].scatter(x=X_train[pred.argmax(dim=1)==k], y=torch.zeros_like(X_train[pred.argmax(dim=1)==k]), color=COLORS[k])\n",
    "\n",
    "        gaussian = MultivariateNormal(loc=mus[k], covariance_matrix=sigmas[k])\n",
    "        y_plot = gaussian.log_prob(value=x_plot.unsqueeze(dim=1)).exp()\n",
    "        ax[i, 0].plot(x_plot.squeeze(dim=1), pis[k]*y_plot.squeeze(dim=1)*5, color=COLORS[k], label=f'Gaussian {k}')\n",
    "    \n",
    "    ax[i, 0].step(x_plot, em.predict(x_plot).argmax(dim=1), color='black', label='Class')\n",
    "\n",
    "    if trainable_pis == True:\n",
    "        ax[i, 0].set(title=f'Trainable Prior ({em.pis})')\n",
    "    else:\n",
    "        ax[i, 0].set(title=f'Fixed Prior ({em.pis})')\n",
    "\n",
    "ax[0, 0].legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">2-D data (Gaussian clusters)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 3\n",
    "NUM_EPOCHS = 100\n",
    "BOUNDARIES = [[-3, 3], [-3, 3]]\n",
    "# For PLOT_STEP = 0.05: RESOLUTION = (MAX - MIN BOUNDARIES)//PLOT_STEP + 1 = 121 --> 121*121 pixel pcolormesh\n",
    "# Smaller PLOT_STEP = more accurate plot, longer inference time\n",
    "PLOT_STEP = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axes_equal(ax):\n",
    "    '''Make axes of 3D plot have equal scale so that spheres appear as spheres,\n",
    "    cubes as cubes, etc.. This is one possible solution to Matplotlib's\n",
    "    ax.set_aspect('equal') and ax.axis('equal') not working for 3D.\n",
    "\n",
    "    Input\n",
    "      ax: a matplotlib axis, e.g., as output from plt.gca().\n",
    "\n",
    "    Self-note: Thanks stranger on the Internet.\n",
    "    '''\n",
    "    x_limits = ax.get_xlim3d()\n",
    "    y_limits = ax.get_ylim3d()\n",
    "    z_limits = ax.get_zlim3d()\n",
    "\n",
    "    x_range = abs(x_limits[1] - x_limits[0])\n",
    "    x_middle = np.mean(x_limits)\n",
    "    y_range = abs(y_limits[1] - y_limits[0])\n",
    "    y_middle = np.mean(y_limits)\n",
    "    z_range = abs(z_limits[1] - z_limits[0])\n",
    "    z_middle = np.mean(z_limits)\n",
    "\n",
    "    # The plot bounding box is a sphere in the sense of the infinity\n",
    "    # norm, hence I call half the max range the plot radius.\n",
    "    plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "    ax.set_box_aspect((1, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EM with 2D Gaussian Mixture, we parameterized the Gaussians with covariance matrices $\\{{\\Sigma^{(k)}}\\}_{k=1}^K$ instead of standard deviation $\\{{\\sigma^{(k)}}\\}_{k=1}^K$.\n",
    "\n",
    "Thus, in the Maximization step, we follow [this formula](https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_covariance) to compute the weighted covariance matrix. (Each sample has a specific weight to the overall covariance - in this case the weight is embedded in the responsibility matrix $R$).\n",
    "- Weighted covariance matrix: $\\Sigma^{(k)} = \\frac{\\left(R_{:, k}\\times(X - \\mu^{(k)})\\right)^T(X - \\mu^{(k)})}{\\sum{R_{:, k}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_2D(\n",
    "    num_clusters:int=2,\n",
    "    radius:float=1,\n",
    "    cluster_scale:float=0.2,\n",
    "    num_examples:int=600,\n",
    ") -> Sequence[Tensor]:\n",
    "    \"\"\"Generates 2D Gaussian clusters evenly spaced around the origin.\n",
    "\n",
    "    Args:\n",
    "    + `num_clusters`: Number of clusters. Defaults to `2`.\n",
    "    + `radius`: Distance of cluster to origin. Defaults to `1`.\n",
    "    + `cluster_scale`: Scale of cluster, i.e., the covariance matrix of each    \\\n",
    "        cluster is `cluster_scale` times identity matrix. Defaults to `0.2`.\n",
    "    + `num_examples`: Number of examples. Defaults to `600`.\n",
    "\n",
    "    Returns:\n",
    "    + Shuffled inputs and labels of data points the clusters.\n",
    "    \"\"\"    \n",
    "    pi = torch.acos(torch.zeros(1))* 2\n",
    "    # mu and Sigma for Gaussian distributions\n",
    "    mus = torch.cat(\n",
    "        [\n",
    "            radius*torch.cos(2*pi*torch.arange(num_clusters)/num_clusters).unsqueeze(dim = 1),\n",
    "            radius*torch.sin(2*pi*torch.arange(num_clusters)/num_clusters).unsqueeze(dim = 1),\n",
    "        ],\n",
    "        dim = 1,\n",
    "    )\n",
    "    Sigmas = (cluster_scale*torch.eye(2)).unsqueeze(dim=0).tile(dims=[num_clusters, 1, 1])\n",
    "\n",
    "    # Pre-allocate x and y\n",
    "    examples_per_cluster = num_examples//num_clusters\n",
    "    x = torch.empty(size=[0, 2])\n",
    "    y = torch.empty(size=[0, 1])\n",
    "    for k in range(num_clusters):\n",
    "        # Sample x from Gaussian distributions\n",
    "        new_x = MultivariateNormal(loc=mus[k], covariance_matrix=Sigmas[k]).sample([examples_per_cluster])\n",
    "        new_y = torch.tensor([[k]]*examples_per_cluster)\n",
    "        x = torch.cat([x, new_x], dim = 0)\n",
    "        y = torch.cat([y, new_y], dim = 0)\n",
    "    # Shuffle data\n",
    "    shuffle = torch.randperm(x.shape[0])\n",
    "    return x[shuffle], y[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Classification for 2D clusters\n",
    "X_train, y_train = get_clusters_2D(num_clusters=NUM_CLUSTERS, radius=1, cluster_scale=0.3, num_examples=600)\n",
    "\n",
    "# Plot all centroids and examples\n",
    "fig, ax = plt.subplots(figsize=(4, 4), constrained_layout=True)\n",
    "fig.suptitle(f'Gaussian clusters')\n",
    "ax.set(xlim=BOUNDARIES[0], ylim=BOUNDARIES[1])\n",
    "\n",
    "for k in range(NUM_CLUSTERS):\n",
    "    ax.scatter(\n",
    "        X_train[y_train.squeeze(dim=1)==k, 0],\n",
    "        X_train[y_train.squeeze(dim=1)==k, 1],\n",
    "        s=3, color=COLORS[k], label=f'Cluster {k}'\n",
    "    )\n",
    "ax.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4), constrained_layout=True)\n",
    "fig.suptitle(f'Gaussian clusters')\n",
    "ax.set(xlim=BOUNDARIES[0], ylim=BOUNDARIES[1])\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], s=3)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4), constrained_layout=True)\n",
    "fig.suptitle(f'Gaussian clusters - K-means')\n",
    "ax.set(xlim=BOUNDARIES[0], ylim=BOUNDARIES[1])\n",
    "# Visualize regions of each class\n",
    "# ptp_X = X_train.max(dim = 0)[0] - X_train.min(dim = 0)[0]\n",
    "# x_plot = torch.arange(X_train[:, 0].min() - 0.2*ptp_X[0], X_train[:, 0].max() + 0.2*ptp_X[1], PLOT_STEP)\n",
    "# y_plot = torch.arange(X_train[:, 1].min() - 0.2*ptp_X[0], X_train[:, 1].max() + 0.2*ptp_X[1], PLOT_STEP)\n",
    "x0_plot = torch.arange(start=BOUNDARIES[0][0], end=BOUNDARIES[0][1]+1e-8, step=PLOT_STEP)#.unsqueeze(dim=1)\n",
    "x1_plot = torch.arange(start=BOUNDARIES[1][0], end=BOUNDARIES[1][1]+1e-8, step=PLOT_STEP)#.unsqueeze(dim=1)\n",
    "x0_grid, x1_grid = torch.meshgrid([x0_plot, x1_plot])\n",
    "X_plot = torch.cat([x0_grid.flatten().unsqueeze(dim=1), x1_grid.flatten().unsqueeze(dim=1)], dim = 1)\n",
    "\n",
    "km = KMeansClassifier(X_train=X_train, K=3)\n",
    "for j in range(NUM_EPOCHS):\n",
    "    y_km = km.forward(X_train=X_train)\n",
    "    km.backward(X_train=X_train, yhat=y_km)\n",
    "y_km = km.forward(X_train)\n",
    "\n",
    "for k in range(km.K):\n",
    "    # Training data\n",
    "    ax.scatter(X_train[(y_km==k).squeeze(dim=1), 0], X_train[(y_km==k).squeeze(dim=1), 1],\n",
    "                color=COLORS[k], alpha=0.7, s=3, zorder=100, label=f'Cluster {k}')\n",
    "    ax.scatter(km.centroids[k][0], km.centroids[k][1],\n",
    "               color=COLORS[k], alpha=1, s=200, marker='x', linewidth=4, zorder=100)\n",
    "\n",
    "y_plot = km.forward(X_plot).reshape([x0_plot.shape[0], x1_plot.shape[0]])\n",
    "ax.pcolormesh(x0_grid.numpy(), x1_grid.numpy(), y_plot.numpy(), cmap=ListedColormap(COLORS[0:km.K]), alpha=0.3, shading = 'auto')\n",
    "ax.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), constrained_layout=True, squeeze=False, sharex='all', sharey='all')\n",
    "fig.suptitle(f'Gaussian clusters - Expectation-Maximization algorithm')\n",
    "ax[0, 0].set(xlim=BOUNDARIES[0], ylim=BOUNDARIES[1])\n",
    "# Visualize regions of each class\n",
    "x0_plot = torch.arange(start=BOUNDARIES[0][0], end=BOUNDARIES[0][1]+1e-8, step=PLOT_STEP)#.unsqueeze(dim=1)\n",
    "x1_plot = torch.arange(start=BOUNDARIES[1][0], end=BOUNDARIES[1][1]+1e-8, step=PLOT_STEP)#.unsqueeze(dim=1)\n",
    "x1, x2 = torch.meshgrid([x0_plot, x1_plot])\n",
    "X_plot = torch.cat([x1.flatten().unsqueeze(dim=1), x2.flatten().unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "for i, trainable_pis in enumerate([True, False]):\n",
    "    em = ExpectationMaximization(\n",
    "        X_train=X_train, K=3, trainable_pis=trainable_pis,\n",
    "        pis=[4, 1, 1],\n",
    "    )\n",
    "    for j in range(NUM_EPOCHS):\n",
    "        r = em.expectation_step()\n",
    "        mus, sigmas, pis = em.maximization_step()\n",
    "    y_em = em.predict(X_train)\n",
    "\n",
    "    for k in range(em.K):\n",
    "        # Training data\n",
    "        ax[0, i].scatter(X_train[(y_em.argmax(dim=1)==k), 0], X_train[(y_em.argmax(dim=1)==k), 1],\n",
    "                    color = COLORS[k], alpha = 0.7, s = 3, zorder = 100, label=f'Gaussian {k}')\n",
    "        ax[0, i].scatter(em.mus[k][0], em.mus[k][1],\n",
    "                color = COLORS[k], alpha = 1, s = 200, marker='x', linewidth=4, zorder = 100)\n",
    "        \n",
    "        # gaussian = MultivariateNormal(loc=mus[k], covariance_matrix=sigmas[k])\n",
    "        # contour = gaussian.pdf(X_plot)\n",
    "        # y_plot = gaussian.log_prob(value=x_plot.unsqueeze(dim=1)).exp()\n",
    "        # ax[i, 0].plot(x_plot.squeeze(dim=1), pis[k]*y_plot.squeeze(dim=1)*5, color=COLORS[k], label=f'Gaussian {k}')\n",
    "        \n",
    "        # ax[i, 0].step(x_plot, em.predict(x_plot).argmax(dim=1), color='black', label='Class')\n",
    "\n",
    "    y_plot = em.predict(X_plot).argmax(dim=1).reshape([x0_plot.shape[0], x1_plot.shape[0]])\n",
    "    ax[0, i].pcolormesh(x0_grid.numpy(), x1_grid.numpy(), y_plot.numpy(), cmap = ListedColormap(COLORS[0:em.K]), alpha = 0.3, shading = 'auto')\n",
    "\n",
    "    if trainable_pis == True:\n",
    "        ax[0, i].set(title=f'Trainable Prior ({em.pis})')\n",
    "    else:\n",
    "        ax[0, i].set(title=f'Fixed Prior ({em.pis})')\n",
    "\n",
    "ax[0, 0].legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), subplot_kw={\"projection\": \"3d\"}, constrained_layout=True, squeeze=False)\n",
    "for i, trainable_pis in enumerate([True, False]):\n",
    "    em = ExpectationMaximization(\n",
    "        X_train=X_train, K=3, trainable_pis=trainable_pis,\n",
    "        # mus=torch.rand(size=(4, 2)),\n",
    "        pis=[4, 1, 1],\n",
    "    )\n",
    "    for j in range(NUM_EPOCHS):\n",
    "        r = em.expectation_step()\n",
    "        mus, sigmas, pis = em.maximization_step()\n",
    "    y_em = em.predict(X_train)\n",
    "\n",
    "    for k in range(em.K):\n",
    "        # Training data\n",
    "        ax[0, i].scatter(\n",
    "            X_train[(y_em.argmax(dim=1)==k), 0], X_train[(y_em.argmax(dim=1)==k), 1],\n",
    "            color=COLORS[k], alpha=0.7, s=3, zorder=100,\n",
    "        )\n",
    "        ax[0, i].scatter(\n",
    "            em.mus[k][0], em.mus[k][1],\n",
    "            color=COLORS[k], alpha=1, s=200, marker='x', linewidth=4, zorder = 100\n",
    "        )\n",
    "        ax[0, i].plot_surface(\n",
    "            x0_grid.numpy(), x1_grid.numpy(), em.pis[k]*em.gaussians[k].log_prob(X_plot).exp().reshape(shape=[x0_plot.shape[0], x1_plot.shape[0]]).numpy()*5,\n",
    "            # color=COLORS[k],\n",
    "            cmap=COLORS[k][0].capitalize() + COLORS[k][1:]+'s',#ListedColormap('white', [COLORS[k]]),\n",
    "            alpha=0.3, label=f'Gassian {k}',\n",
    "        )\n",
    "        ax[0, i].contour3D(\n",
    "            x0_grid.numpy(), x1_grid.numpy(), em.pis[k]*em.gaussians[k].log_prob(X_plot).exp().reshape(shape=[x0_plot.shape[0], x1_plot.shape[0]]).numpy()*5,\n",
    "            levels=5, cmap=ListedColormap(COLORS[k], [COLORS[k]]), label=f'Gaussian {k}'\n",
    "        )\n",
    "\n",
    "    set_axes_equal(ax[0, i])\n",
    "    if trainable_pis == True:\n",
    "        ax[0, i].set(title=f'Trainable Prior ({em.pis})')\n",
    "    else:\n",
    "        ax[0, i].set(title=f'Fixed Prior ({em.pis})')\n",
    "\n",
    "ax[0, 0].shareview(ax[0, 1])\n",
    "ax[0, 0].legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
