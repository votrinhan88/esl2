{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Literal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'figure.titlesize': 12,\n",
    "    'axes.titlesize':   10,\n",
    "    'axes.labelsize':   10,\n",
    "    'font.size':        8,\n",
    "    'xtick.labelsize':  8,\n",
    "    'ytick.labelsize':  8,\n",
    "    'legend.fontsize':  8,\n",
    "    'lines.linewidth':  1,\n",
    "})\n",
    "\n",
    "COLORS = ['red', 'blue', 'green', 'orange', 'purple',\n",
    "          'brown', 'pink', 'gray', 'olive', 'cyan',\n",
    "          'tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:purple',\n",
    "          'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">Tree-based Methods</font>\n",
    "\n",
    "A Decision Tree splits the source dataset into subsets. This splitting process is repeated on each derived subset in a recursive and greedy manner. The recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions.\n",
    "\n",
    "<font size=\"+2\">How to split</font>:  \n",
    "Reducing the variance (for regression) or the impurity (for classification) of the data.\n",
    "\n",
    "Following the book, this notebook explores the typical tree model: Classification and Regression Tree (CART)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">Regression Trees</font>  \n",
    "\n",
    "<font size=\"+3\">Problem formulation</font>  \n",
    "\n",
    "Given data of inputs $\\mathbf{X}_{N\\times p}$ (numerical or categorical) and labels $\\mathbf{Y} \\in \\mathbb{R}^N$.  \n",
    "The decision tree $T$ splits our data into $M$ regions $R_1, R_2, \\ldots, R_M$.\n",
    "\n",
    "At each node $m$ corresponding to the region $R_m$, the prediction is modelled as a constant $c_m$ that can represent the node's population.\n",
    "$$f(x) = \\sum_{m=1}^M c_m \\mathbf{I}(x\\in R_m)$$\n",
    "\n",
    "Where $c_m$ can be straightforwardly be modelled as the average of $N_m$ samples in the current population at $R_m$.\n",
    "$$\\hat{c}_m = \\frac{1}{N_m}\\sum_{x_i \\in R_m}y_i$$\n",
    "Note that in this case, $c_m$ also minimizes the squared errors of the predictions $\\sum\\left(y_i - f(x_i)\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">Splitting criteria</font>  \n",
    "\n",
    "Consider a split with the $j$-th input, $j \\in \\{1, 2, \\ldots, p\\}$, at split point $s \\in \\mathbb{R}$. That is splitting a region $R$ into two half-planes:\n",
    "+ $R_\\text{L}(j, s) = \\{X|X_j \\le s\\}$\n",
    "+ $R_\\text{R}(j, s) = \\{X|X_j > s\\}$\n",
    "\n",
    "We seek to split via *variance reduction*, i.e., finding a split such that:\n",
    "$$\\min_{j, s} \\left[\\min_{c_\\text{L}} \\sum_{x_i\\in R_\\text{L}(j, s)}(y_i - c_\\text{L})^2 +\n",
    "                    \\min_{c_\\text{R}} \\sum_{x_i\\in R_\\text{R}(j, s)}(y_i - c_\\text{R})^2 \\right]$$\n",
    "\n",
    "The inner minimization is acquired when $c_\\text{L}, c_\\text{R}$ is the average labels, $\\forall (j, s)$.\n",
    "The outer minimization is done by looping through every possible $j$'s and $s$'s.  \n",
    "\n",
    "Pseudo-code for finding a split at node $m$:\n",
    "+ **FOR** $j \\coloneqq 1$ to $p$:\n",
    "    + **FOR** $s \\coloneqq x_1$ to $x_{N_m}$:\n",
    "        + **COMPUTE** $X_\\text{L}, X_\\text{R}$\n",
    "        + **COMPUTE** $c_\\text{L}, c_\\text{R}$\n",
    "        + **COMPUTE** sum of variance $S(j, s) = \\sum_{x_i\\in R_\\text{L}(j, s)}(y_i - c_\\text{L})^2 + \\sum_{x_i\\in R_\\text{R}(j, s)}(y_i - c_\\text{R})^2$\n",
    "+ **RETURN** $(j, s)$ with the lowest $S(j, s)$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">Classification Trees</font>  \n",
    "\n",
    "<font size=\"+3\">Problem formulation</font>  \n",
    "\n",
    "Given data of inputs $\\mathbf{X}_{N\\times p}$ (numerical or categorical) and labels $\\mathbf{Y} \\in \\{1, 2, \\ldots, K\\}^N$.  \n",
    "The decision tree $T$ splits our data into $M$ regions $R_1, R_2, \\ldots, R_M$.\n",
    "\n",
    "At each node $m$ corresponding to the region $R_m$, the prediction is modelled as a probability distribution $\\hat{p}_m$ can represent the node's population of $N_m$ samples. For class $k$:\n",
    "$$\\hat{p}_{m, k} = \\frac{1}{N_m}\\sum_{x_i \\in R_m}\\mathbf{I}(y_i = k)$$\n",
    "We can choose either quantity to represent the population and make prediction:\n",
    "+ Probability distribution: $f(x) = \\hat{p}_{m, k}$\n",
    "+ Major class: $f(x) = \\argmax_k \\hat{p}_{m, k} = k(m)$ (more popular and lawful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">Splitting criteria</font>  \n",
    "\n",
    "Consider a split with the $j$-th input, $j \\in \\{1, 2, \\ldots, p\\}$, at split point $s \\in \\mathbb{R}$. That is splitting a region $R$ into two half-planes:\n",
    "+ $R_\\text{L}(j, s) = \\{X|X_j \\le s\\}$\n",
    "+ $R_\\text{R}(j, s) = \\{X|X_j > s\\}$\n",
    "\n",
    "We seek to split via improving the *purity* of the sub-regions, i.e., they are efficently grouped to separate classes.\n",
    "\n",
    "We can measure the **Impurity** (smaller is better) of each partition $r \\in \\{R_\\text{L}, R_\\text{R}\\}$ with probability distribution $\\hat{p}$ by:\n",
    "+ Misclassification error: $$Q(r) = \\frac{1}{|r|}\\sum_{x_i \\in r}I(y_i \\neq k)$$\n",
    "+ Gini index: $$Q(r) = \\sum_{k=1}^K \\sum_{k'\\neq k} \\hat{p}_{k} \\cdot \\hat{p}_{k'} = \\sum_{k=1}^K \\hat{p}_{k} \\cdot (1 - \\hat{p}_{k})$$\n",
    "+ Cross-entropy: $$Q(r) = -\\sum_{k=1}^K \\hat{p}_{k} \\log \\hat{p}_{k}$$\n",
    "Then impurity measure of such split can be computed by the weighted average of partitions.\n",
    "$$\\text{GI}(\\text{split}) = \\frac{|R_\\text{L}|}{|R|}\\times Q(R_\\text{L}) + \\frac{|R_\\text{R}|}{|R|}\\times Q(R_\\text{R})$$\n",
    "We want split that can give purest partitions, i.e., lowest impurity: $\\min\\left[ \\frac{|R_\\text{L}|}{|R|}\\times Q(R_\\text{L}) + \\frac{|R_\\text{R}|}{|R|}\\times Q(R_\\text{R}) \\right]$\n",
    "\n",
    "Another splitting criteria is Information Gain:\n",
    "+ Inspired by Information Entropy: $H(\\hat{p}) = -\\sum_{k=1}^K \\hat{p}_{k} \\log_2 \\hat{p}_{k}$\n",
    "+ The purer $\\hat{p}$ is, the more predictable its outcomes are, the lower entropy $H(\\hat{p})$.\n",
    "+ We want split that can gain the most information, i.e.: $\\max\\left[ H(\\hat{p}_R) - \\left( \\frac{|R_\\text{L}|}{|R|}\\times H(\\hat{p}_{R_\\text{L}}) + \\frac{|R_\\text{R}|}{|R|}\\times H(\\hat{p}_{R_\\text{R}}) \\right) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Splitting $X = \\{\\triangle, \\triangle, \\square, \\square, \\bigcirc, \\bigcirc, \\bigcirc\\}$ and compute Gini index:\n",
    "+ Case 1: $X_\\text{L} = \\{\\triangle, \\square, \\bigcirc\\}$ and $X_\\text{R} = \\{\\triangle, \\square, \\bigcirc, \\bigcirc\\}$\n",
    "    + $\\text{GI}(X_\\text{L}) = \\frac{1}{3}\\times\\frac{2}{3}\\times 3 = \\frac{2}{3}$\n",
    "    + $\\text{GI}(X_\\text{R}) = \\frac{1}{4}\\times\\frac{3}{4}\\times 2 + \\frac{1}{2}\\times\\frac{1}{2} = \\frac{5}{8}$\n",
    "    + $\\text{GI}(\\text{split}) = \\frac{3}{7}\\times\\frac{2}{3} + \\frac{4}{7}\\times\\frac{5}{8} \\approx 0.643$\n",
    "+ Case 2: $X_\\text{L} = \\{\\triangle, \\bigcirc, \\bigcirc\\}$ and $X_\\text{R} = \\{\\triangle, \\square, \\square, \\bigcirc\\}$\n",
    "    + $\\text{GI}(X_\\text{L}) = \\frac{1}{3}\\times\\frac{2}{3} + \\frac{2}{3}\\times\\frac{1}{3} = \\frac{4}{9}$\n",
    "    + $\\text{GI}(X_\\text{R}) = \\frac{1}{4}\\times\\frac{3}{4}\\times 2 + \\frac{1}{2}\\times\\frac{1}{2} = \\frac{5}{8}$\n",
    "    + $\\text{GI}(\\text{split}) = \\frac{3}{7}\\times\\frac{4}{9} + \\frac{4}{7}\\times\\frac{5}{8} \\approx 0.548$\n",
    "+ Case 3: $X_\\text{L} = \\{\\triangle, \\square\\}$ and $X_\\text{R} = \\{\\triangle, \\square, \\bigcirc, \\bigcirc, \\bigcirc\\}$\n",
    "    + $\\text{GI}(X_\\text{L}) = \\frac{1}{2}\\times\\frac{1}{2}\\times 2 = \\frac{1}{2}$\n",
    "    + $\\text{GI}(X_\\text{R}) = \\frac{1}{5}\\times\\frac{4}{5}\\times 2 + \\frac{3}{5}\\times\\frac{2}{5} = \\frac{14}{25}$\n",
    "    + $\\text{GI}(\\text{split}) = \\frac{2}{7}\\times\\frac{1}{2} + \\frac{5}{7}\\times\\frac{14}{25} \\approx 0.543$\n",
    "\n",
    "With the smallest Gini index, case 3 is the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">Regularization</font>  \n",
    "\n",
    "+ Threshold-based: Stop growing tree if no possible split can reduce variance (for regression) or impurity (for classification) beyond a specified threshold.\n",
    "+ Complexity-based:\n",
    "    + Limit the number of nodes\n",
    "    + Limit the tree depth\n",
    "    + Cost-complexity to adaptively prune the tree: $C_\\alpha = \\sum_{m=1}^{|T|} [N_m Q(m)] + \\alpha\\cdot |T|$\n",
    "        + $\\alpha$: specified regularization parameter\n",
    "        + $|T|$: number of terminal nodes (leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">Advantages & Disadvantages</font>  \n",
    "\n",
    "(+):\n",
    "+ Interpretability, resembles the human-thinking process. Widely used in medicine.\n",
    "+ Works for both regression and classification tasks.\n",
    "+ Works for both numerical and categorical data.\n",
    "+ Minimal data preparation: no need to normalize data or prepare dummy variables (e.g., one-hot encoding)\n",
    "+ Non-parametric, makes no assumption on the data (but only works well for data separable by constants)\n",
    "+ In-build feature selection (closer to root = more important)\n",
    "\n",
    "(-):\n",
    "+ High variance, overfitting. Because of its greedy and recursive manner, error can propagates from early to late regions.\n",
    "    + Solution: Bagging (e.g., Random Forest).\n",
    "+ Greedy: may miss global optimum\n",
    "+ For data including categorical inputs with different number of classes, Information Gain is biased towards the inputs with more classes \n",
    "+ Lack of smoothness, lack of additive structure\n",
    "    + Solution: Multivariate adaptive regression spline (MARS), trade-off interpretability with performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+5\">Other Issues</font>  \n",
    "\n",
    "+ Categorical Predictors: Large number of classes makes trees more prone to overfitting\n",
    "+ Loss matrix: The cost of misclassication can be customed to a pre-defined loss matrix. Suitable for case when one outcome is critically avoided (e.g., disease vs. non-disease).\n",
    "+ Semi-supervised problem (missing predictor values): Two approaches:\n",
    "    + Categorize samples with missing values as a separate class, therefore finding the pattern of these samples\n",
    "    + Surrogate predictors\n",
    "+ Why binary splits? Avoid fragmenting the data too quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, task:Literal['r', 'c'] = 'c',\n",
    "                       depth:int = None,\n",
    "                       path:str = '',\n",
    "                       parent = None):\n",
    "        # Arguments\n",
    "        self.task        = task\n",
    "        self.depth       = depth\n",
    "        self.path        = path\n",
    "        self.parent:Node = parent\n",
    "        # Pre-allocate\n",
    "        self.is_leaf = False\n",
    "        self.info:Tensor = None\n",
    "        self.feature:Tensor = None\n",
    "        self.threshold:Tensor = None\n",
    "        self.children:Sequence[Node] = []\n",
    "        # Based on task (Classification/Regression)\n",
    "        self.value:Tensor = None\n",
    "        if self.task == 'c':\n",
    "            self.distr:Tensor = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Magic attribute to get class name\n",
    "        if self.depth == 0:\n",
    "            return f\"{self.__class__.__name__}-{self.task} Root, I = {self.info:.3f}, value = {round(self.value, 2)}\"\n",
    "        elif self.depth > 0:\n",
    "            return f\"{self.__class__.__name__}-{self.task} {self.path}, I = {self.info:.3f}, value = {round(self.value, 2)}\"\n",
    "\n",
    "    def branch(self):\n",
    "        left_node  = Node(task = self.task, depth = self.depth + 1, path = self.path + 'L', parent = self)\n",
    "        right_node = Node(task = self.task, depth = self.depth + 1, path = self.path + 'R', parent = self)\n",
    "        return left_node, right_node\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, task:Literal['c', 'r'] = 'c',\n",
    "                       method_info:Literal['gini', 'entropy', 'var', 'std'] = 'gini',\n",
    "                       max_depth:int = 4,\n",
    "                       drop_features:bool = True):\n",
    "        assert ((task == 'c') & (method_info in ['gini', 'entropy']) |\n",
    "                (task == 'r') & (method_info in ['var', 'std'])), \\\n",
    "               \"Use task 'c' with method_info 'gini' or 'entropy', or task 'r' with method_info 'var' or 'std'\"\n",
    "        self.task          = task\n",
    "        self.method_info   = method_info\n",
    "        self.max_depth     = max_depth\n",
    "        self.drop_features = drop_features\n",
    "\n",
    "        self.depth:int = 0\n",
    "        self.feature_picks:Tensor = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"DT-{self.task} with depth {self.depth} (max {self.max_depth}), drop features {'ON' if self.drop_features else 'OFF'}\"\n",
    "\n",
    "    def fit(self, X_train:Tensor, y_train:Tensor):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.num_features = X_train.size()[1]\n",
    "        self.feature_picks = torch.arange(self.num_features)\n",
    "\n",
    "        self.root = Node(depth = 0, task = self.task)\n",
    "\n",
    "        if self.task == 'c':\n",
    "            self.num_classes = len(y_train.unique())\n",
    "            self.feed = self.feed_c\n",
    "            self.compute_info = self.compute_info_c\n",
    "        elif self.task == 'r':\n",
    "            self.feed = self.feed_r\n",
    "            self.compute_info = self.compute_info_r\n",
    "\n",
    "        idx_root = torch.arange(self.X_train.size()[0])\n",
    "        self.feed(node = self.root,\n",
    "                  idx_label = idx_root)\n",
    "        self.grow(node = self.root,\n",
    "                  idx_node = idx_root)\n",
    "\n",
    "    def grow(self, node:Node, idx_node:Tensor):\n",
    "        max_gain = self.find_best_split(node, idx_node)\n",
    "        if max_gain > 0:\n",
    "            # Continue growing\n",
    "            self.depth = max(self.depth, node.depth + 1)\n",
    "            left_node, right_node = node.branch()\n",
    "            node.children = [left_node, right_node]\n",
    "            # Re-split data by optimal feature and threshold for children nodes\n",
    "            idx_left, idx_right = self.split(idx_node, feature = node.feature, threshold = node.threshold)\n",
    "            # Leaf node:\n",
    "            #  - Has a unique class distribution (contains only one class)\n",
    "            #  - OR has reached max depth\n",
    "            for (branch, idx_branch) in zip([left_node, right_node], [idx_left, idx_right]):\n",
    "                self.feed(branch, idx_branch)\n",
    "                y_branch = self.y_train[idx_branch]\n",
    "                if (len(y_branch.unique()) == 1) | (branch.depth == self.max_depth):\n",
    "                    branch.is_leaf = True\n",
    "                else:\n",
    "                    self.grow(branch, idx_branch)\n",
    "    \n",
    "    def find_best_split(self, node:Node, idx_node:Tensor) -> Tensor:\n",
    "        X_node = self.X_train[idx_node]\n",
    "        # Select random features (sqrt() of previous node's num_features)\n",
    "        if self.drop_features == True:\n",
    "            shuffle_idx = torch.randperm(self.feature_picks.numel())\n",
    "            self.feature_picks = self.feature_picks.view(-1)[shuffle_idx].view(self.feature_picks.size())\n",
    "            self.feature_picks = self.feature_picks[0:int(self.feature_picks.numel()**0.5)]\n",
    "        # Split based on the reduced (or not) set of features \n",
    "        max_gain = -torch.tensor(float('inf'))\n",
    "        for feature in torch.arange(self.num_features):\n",
    "            # thresholds = torch.linspace(start = X_node[:, feature].min(),\n",
    "            #                             end = X_node[:, feature].max(),\n",
    "            #                             steps = self.num_splits + 2)[1:self.num_splits+1]\n",
    "            uniques = X_node[:, feature].sort()[0].unique()\n",
    "            thresholds = (uniques[1:] + uniques[:-1])/2\n",
    "            for threshold in thresholds:\n",
    "                idx_left, idx_right = self.split(idx_node = idx_node,\n",
    "                                                 feature = feature,\n",
    "                                                 threshold = threshold)\n",
    "                gain = self.compute_gain(node,\n",
    "                                         idx_node = idx_node,\n",
    "                                         idx_left = idx_left,\n",
    "                                         idx_right = idx_right)\n",
    "                if gain > max_gain:\n",
    "                    max_gain = gain\n",
    "                    opt_feature = feature\n",
    "                    opt_threshold = threshold        \n",
    "        # Update node with optimal split\n",
    "        node.feature = opt_feature\n",
    "        node.threshold = opt_threshold\n",
    "        return max_gain\n",
    "\n",
    "    def split(self, idx_node:Tensor, feature:Tensor, threshold:Tensor) -> Sequence[Tensor]:\n",
    "        X_node = self.X_train[idx_node]\n",
    "\n",
    "        left_ind = X_node[:, feature] <= threshold\n",
    "        idx_left = idx_node[left_ind]\n",
    "        idx_right = idx_node[~left_ind]\n",
    "        return idx_left, idx_right\n",
    "\n",
    "    def compute_gain(self, node:Node, idx_node:Tensor, idx_left:Tensor, idx_right:Tensor) -> Tensor:\n",
    "        left_info = self.compute_info(idx_left)\n",
    "        right_info = self.compute_info(idx_right)\n",
    "        \n",
    "        gain = node.info - (idx_left.numel()*left_info + idx_right.numel()*right_info)/idx_node.numel()\n",
    "        \n",
    "        # w_node = self.weights[idx_node].sum()\n",
    "        # w_left = self.weights[idx_left].sum()\n",
    "        # w_right = self.weights[idx_right].sum()\n",
    "        # gain = w_node*node.info - (w_left/w_node*left_info + w_right/w_node*right_info)\n",
    "        return gain\n",
    "\n",
    "    def feed_c(self, node:Node, idx_label:Tensor):\n",
    "        label = self.y_train[idx_label]\n",
    "        node.info = self.compute_info_c(idx_label)\n",
    "        onehot:Tensor = F.one_hot(label.squeeze(dim = 1), num_classes = self.num_classes)\n",
    "        \n",
    "        node.distr = onehot.sum(dim = 0)\n",
    "\n",
    "        # node.distr = (self.weights[idx_label]*onehot).sum(dim = 0)\n",
    "        # node.distr = node.distr/node.distr.sum()\n",
    "\n",
    "        node.value = node.distr.max(dim = 0)[1]\n",
    "\n",
    "    def feed_r(self, node:Node, idx_label:Tensor):\n",
    "        label = self.y_train[idx_label]\n",
    "        node.info = self.compute_info_r(idx_label)\n",
    "        node.value = label.mean()\n",
    "    \n",
    "    def compute_info_c(self, idx_label:Tensor) -> Tensor:\n",
    "        label = self.y_train[idx_label]\n",
    "        \n",
    "        # Classification: reducing Gini impurity or entropy\n",
    "        onehot:Tensor = F.one_hot(label.squeeze(dim = 1), num_classes = self.num_classes)\n",
    "        distr = onehot.sum(dim = 0)\n",
    "        distr = distr/distr.sum()\n",
    "        if self.method_info == 'gini':\n",
    "            info = 1 - (distr**2).sum()\n",
    "        elif self.method_info == 'entropy':\n",
    "            # Side note:\n",
    "            # 1. Entropy of a system (all classes) = sum of entropy of its parts (each class)\n",
    "            # 2. Moreover, if a class has no examples, it is absolutely predictable absent, hence its\n",
    "            #   entropy is 0.\n",
    "            # 3. To ease dealing with absent classes (which yields log(0) = -inf), and (2.), the \n",
    "            #   computation only considers existing classes.\n",
    "            info = -(distr[distr != 0]*distr[distr != 0].log()).sum()\n",
    "        return info\n",
    "    \n",
    "    def compute_info_r(self, idx_label:Tensor) -> Tensor:\n",
    "        label = self.y_train[idx_label]\n",
    "\n",
    "        # Regression: reducing Variance or Standard deviation\n",
    "        if self.method_info == 'var':\n",
    "            info = ((label - label.mean())**2).sum()/label.size()[0]\n",
    "        elif self.method_info == 'std':\n",
    "            info = ((label - label.mean())**2).sum().sqrt()/label.size()[0]\n",
    "        return info\n",
    "\n",
    "    def print_tree(self):\n",
    "        def traverse_print(node: Node):\n",
    "            # Print node\n",
    "            if node.depth == 0:\n",
    "                print(f\"Root:\")\n",
    "            elif node.depth > 0:\n",
    "                print(f\"{'    '*node.depth}Branch {node.path} \" +\n",
    "                      f\"(x{node.parent.feature.item()} {'â‰¤' if node.path[-1] == 'L' else '>'} {node.parent.threshold.item():.2f}):\" +\n",
    "                      f\"{f' {node.distr.numpy()}' if self.task == 'c' else ''}\" +\n",
    "                      f\"{f' = {round(node.value.item(), 2)}' if node.is_leaf else ''}\")\n",
    "            # Go to children branches\n",
    "            if node.is_leaf == False:\n",
    "                for branch in node.children:\n",
    "                    traverse_print(branch)\n",
    "            \n",
    "        traverse_print(self.root)\n",
    "\n",
    "    def forward(self, input:Tensor, method = 'all') -> Tensor:\n",
    "        if method == 'each':\n",
    "            # Method 1: Loop and traverse each example through the tree\n",
    "            def traverse_forward(node:Node, input:Tensor, yhat) -> Tensor:\n",
    "                if yhat is not None:\n",
    "                    return yhat\n",
    "                elif yhat is None:\n",
    "                    if node.is_leaf == True:\n",
    "                        return node.value\n",
    "                    elif node.is_leaf == False:\n",
    "                        if input[node.feature] < node.threshold:\n",
    "                            return traverse_forward(node.children[0], input, yhat)\n",
    "                        elif input[node.feature] >= node.threshold:\n",
    "                            return traverse_forward(node.children[1], input, yhat)\n",
    "            \n",
    "            yhat = -torch.ones([input.size()[0], 1], dtype={'c':torch.long, 'r':torch.float}.get(self.task))\n",
    "            for example in torch.arange(input.size()[0]):\n",
    "                yhat[example] = traverse_forward(self.root, input[example, :], yhat = None)\n",
    "            return yhat\n",
    "\n",
    "        elif method == 'all':\n",
    "            # Method 2: Traverse all examples through the tree at once\n",
    "            def traverse_forward(node:Node, input:Tensor, yhat:Tensor, yhat_id:Tensor) -> Tensor:\n",
    "                if node.is_leaf == True:\n",
    "                    yhat[yhat_id.squeeze()] = node.value\n",
    "                    return yhat\n",
    "                elif node.is_leaf == False:\n",
    "                    left_ind = input[:, node.feature] < node.threshold\n",
    "                    left_input, right_input = input[left_ind, :], input[~left_ind, :]\n",
    "                    idx_left, idx_right = yhat_id[left_ind, :], yhat_id[~left_ind, :]\n",
    "                    for branch, branch_input, idx_branch in zip(node.children, [left_input, right_input], [idx_left, idx_right]):\n",
    "                        if len(idx_branch) > 0:\n",
    "                            yhat = traverse_forward(branch, branch_input, yhat, idx_branch)\n",
    "                    return yhat\n",
    "\n",
    "            yhat = -torch.ones([input.size()[0], 1], dtype={'c':torch.long, 'r':torch.float}.get(self.task))\n",
    "            yhat_id = torch.arange(input.size()[0]).unsqueeze(dim = 1)\n",
    "            yhat = traverse_forward(self.root, input, yhat, yhat_id)\n",
    "            return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import get_iris\n",
    "\n",
    "# Task 1: Classification on Iris\n",
    "def expt_iris(shuffle:bool=True, train_test_split:float=0.5):\n",
    "    print('Classification Tree on Iris dataset.')\n",
    "    print(f'Shuffle: {shuffle}')\n",
    "    print(f'Train-test split: {train_test_split}')\n",
    "    print()\n",
    "    # Load Iris\n",
    "    X, Y = get_iris(shuffle=shuffle)\n",
    "    num_trains = round(X.size()[0]*train_test_split)\n",
    "    X_train, y_train = X[0:num_trains], Y[0:num_trains]\n",
    "    X_test, y_test = X[num_trains:], Y[num_trains:]\n",
    "\n",
    "    h = DecisionTree(task='c', method_info='gini', max_depth=3, drop_features=True)\n",
    "    h.fit(X_train, y_train)\n",
    "    yhat = h.forward(X_test)\n",
    "    print(h)\n",
    "    h.print_tree()\n",
    "    print(f'Accuracy = {((yhat == y_test).sum()/y_test.size()[0]).item():.4f}')\n",
    "\n",
    "expt_iris(shuffle=True, train_test_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Regression on 1D Signal\n",
    "def expt_1d_signal(num_observed:int=20):\n",
    "    print('Regression Tree on 1D signal.')\n",
    "    print(f'Number of observed values: {num_observed}')\n",
    "    # Generate dummy data\n",
    "    def signal(input):\n",
    "        return 0.6*input + torch.sin(input)\n",
    "    X_train = 10*torch.rand([num_observed, 1])\n",
    "    y_train = signal(X_train)\n",
    "\n",
    "    h = DecisionTree(task='r', method_info='var', max_depth=3, drop_features=True)\n",
    "    h.fit(X_train, y_train)\n",
    "\n",
    "    X_test = torch.arange(start=-2, end=12, step=0.01).unsqueeze(dim = 1)\n",
    "    y_test = h.forward(X_test)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), constrained_layout=True, squeeze=False)\n",
    "    ax[0, 0].scatter(X_train, y_train,\n",
    "            color='black', s=40, label='Observed')\n",
    "    ax[0, 0].step(X_test, y_test,\n",
    "            linewidth=2, color='blue', label='DT prediction')\n",
    "    ax[0, 0].plot(X_test, signal(X_test),\n",
    "            linewidth=1, linestyle='dashed', color='red', alpha=0.5, label='Ground truth')\n",
    "    ax[0, 0].set(title = h)\n",
    "\n",
    "    ax[0, 0].legend()\n",
    "    h.print_tree()\n",
    "    plt.show()\n",
    "\n",
    "expt_1d_signal(num_observed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet to benchmarking tree.forward() using method = 'each' vs 'all'\n",
    "# Load Iris\n",
    "X, Y = get_iris(shuffle=True)\n",
    "num_trains = round(X.size()[0]*0.5)\n",
    "X_train, y_train = X[0:num_trains], Y[0:num_trains]\n",
    "X_test, y_test = X[num_trains:], Y[num_trains:]\n",
    "\n",
    "h = DecisionTree(task='c', method_info='gini', max_depth=4, drop_features=True)\n",
    "h.fit(X_train, y_train)\n",
    "yhat = h.forward(X_test)\n",
    "print(h)\n",
    "h.print_tree()\n",
    "print(f'Accuracy = {((yhat == y_test).sum()/y_test.size()[0]).item():.4f}')\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    yhat_each = h.forward(X_test, method='each')\n",
    "end = time.time()\n",
    "time1 = end - start\n",
    "print(f'Forwarding examples one-by-one: {time1:.2e} s')\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    yhat_all = h.forward(X_test, method='all')\n",
    "end = time.time()\n",
    "time2 = end - start\n",
    "print(f'Forwarding examples all at once: {time2:.2e} s')\n",
    "\n",
    "print(f'Faster how many times? {time1/time2:.2f}')\n",
    "print(f'Confirm results of both methods are the same: {(yhat_each == yhat_all).prod().bool().item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
